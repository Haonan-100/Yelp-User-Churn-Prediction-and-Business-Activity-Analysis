"""
Model Training & Evaluation for Yelp Userâ€‘Churn Prediction
----------------------------------------------------------
Converted from `3_model_training.ipynb` so it can be executed as a
standâ€‘alone script (CLIâ€‘friendly & CIâ€‘friendly).

Usage
-----
Run from project root:
    python -m src.model_training

Prerequisites
-------------
* `user_churn_features.parquet`, `train_user_ids.txt`, `test_user_ids.txt`
  generated by `src/user_churn_features.py` must exist in
  `data/processed/`.
* XGBoost â‰¥ 1.7 installed.
"""

# 0 | Imports & paths ---------------------------------------------------------
from pathlib import Path
import pandas as pd
import xgboost as xgb
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
)
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

PROC_DIR   = Path("../data/processed")
MODEL_DIR  = Path("../models"); MODEL_DIR.mkdir(exist_ok=True, parents=True)


# 1 | Load feature table ------------------------------------------------------
print("ðŸ”¹ Loading engineered features â€¦")
feat_df = pd.read_parquet(PROC_DIR / "user_churn_features.parquet")

# Basic cleaning: drop date col, cast boolâ†’int
if "last_review_date" in feat_df.columns:
    feat_df = feat_df.drop(columns=["last_review_date"])

bool_cols = feat_df.select_dtypes("bool").columns
feat_df[bool_cols] = feat_df[bool_cols].astype("int8")

# Sanity check: only numeric after removing identifiers/label
non_numeric = feat_df.drop(columns=["user_id", "churn_label"]).select_dtypes(exclude=["number"])
assert not non_numeric.columns.tolist(), f"Nonâ€‘numeric columns found: {non_numeric.columns.tolist()}"


# 2 | Train / test split ------------------------------------------------------
print("ðŸ”¹ Reading preâ€‘defined train/test user lists â€¦")
train_ids = pd.read_csv(PROC_DIR / "train_user_ids.txt", header=None)[0]
test_ids  = pd.read_csv(PROC_DIR / "test_user_ids.txt",  header=None)[0]

train_df = feat_df[feat_df["user_id"].isin(train_ids)].copy()
test_df  = feat_df[feat_df["user_id"].isin(test_ids)].copy()

X_train, y_train = train_df.drop(columns=["user_id", "churn_label"]), train_df["churn_label"]
X_test,  y_test  = test_df .drop(columns=["user_id", "churn_label"]),  test_df ["churn_label"]

print(f"Training rows: {len(X_train):,}  (pos={y_train.sum():,})")
print(f"Testing  rows: {len(X_test):,}   (pos={y_test.sum():,})")

# Further split to obtain a validation set for early stopping
X_tr, X_val, y_tr, y_val = train_test_split(
    X_train, y_train, test_size=0.1, stratify=y_train, random_state=42
)

# 3 | Build DMatrix objects ---------------------------------------------------
dtrain = xgb.DMatrix(X_tr,  label=y_tr)
dval   = xgb.DMatrix(X_val, label=y_val)
dtest  = xgb.DMatrix(X_test, label=y_test)

# 4 | Train model -------------------------------------------------------------
print("ðŸ”¹ Training XGBoost model â€¦ (this may take a while)")
pos, neg = y_tr.sum(), len(y_tr) - y_tr.sum()
params = {
    "objective":        "binary:logistic",
    "eval_metric":      "auc",
    "eta":              0.08,
    "max_depth":        6,
    "subsample":        0.8,
    "colsample_bytree": 0.8,
    "scale_pos_weight": float(neg) / float(pos),
    "seed":             42,
}

watch_list = [(dtrain, "train"), (dval, "val")]
model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=800,
    evals=watch_list,
    early_stopping_rounds=50,
    verbose_eval=50,
)

# 5 | Evaluation --------------------------------------------------------------
print("ðŸ”¹ Evaluating on test set â€¦")
proba = model.predict(dtest, iteration_range=(0, model.best_iteration + 1))
pred  = (proba >= 0.5).astype(int)

metrics = {
    "accuracy":  accuracy_score(y_test, pred),
    "precision": precision_score(y_test, pred),
    "recall":    recall_score(y_test, pred),
    "f1":        f1_score(y_test, pred),
    "auc":       roc_auc_score(y_test, proba),
}

for k, v in metrics.items():
    print(f"{k.upper():9} = {v:.4f}")

# 6 | Save model & plot importance -------------------------------------------
model_path = MODEL_DIR / "churn_xgb_model.json"
model.save_model(model_path)
print("âœ… Model saved to", model_path)

print("ðŸ”¹ Plotting feature importance â€¦")
ax = xgb.plot_importance(model, max_num_features=10)
ax.figure.suptitle("Topâ€‘10 Feature Importance")
plt.tight_layout()
plt.show()


if __name__ == "__main__":
    # All actions already performed above when script is executed.
    pass
